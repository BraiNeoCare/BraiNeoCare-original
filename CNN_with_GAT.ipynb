{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np \n",
    "from scipy import signal \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_addons as tfa\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('../BraiNeoCare/Datasets/GAT/traindata.npy',mmap_mode='r')\n",
    "y_train = np.load('../BraiNeoCare/Datasets/GAT/trainlabels.npy',mmap_mode='r')\n",
    "x_test  = np.load('../BraiNeoCare/Datasets/GAT/testdata.npy',mmap_mode='r')\n",
    "y_test  = np.load('../BraiNeoCare/Datasets/GAT/testlabels.npy',mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=x_train.mean()\n",
    "std=x_train.std()\n",
    "\n",
    "x_train=(x_train-mean)/std\n",
    "x_test=(x_test-mean)/std\n",
    "\n",
    "train_Aug = np.flip(x_train,axis=2)\n",
    "for r in range(train_Aug.shape[0]):\n",
    "    random_noise=np.random.normal(0,0.001,(12,384))\n",
    "    train_Aug[r]+=random_noise\n",
    "x_train=np.expand_dims(x_train,axis=-1)\n",
    "x_test=np.expand_dims(x_test,axis=-1)\n",
    "# x1=np.expand_dims(x1,axis=-1)\n",
    "\n",
    "np.random.seed(25)\n",
    "train_indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(train_indices)\n",
    "x_train = x_train[train_indices]\n",
    "y_train = y_train[train_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for 18 channels \n",
    "adj=tf.constant(\n",
    "    [  [1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "       [0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "       [1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
    "       [0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
    "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.],\n",
    "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
    "    dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names=[\"Fp1-T3\",\"T3-O1\",\"Fp1-C3\",\"C3-O1\",\"Fp2-C4\",\"C4-O2\",\"Fp2-T4\",\"T4-O2\",\"T3-C3\",\"C3-Cz\",\"Cz-C4\",\"C4-T4\"]\n",
    "indices =[[r,i] for r,c1 in enumerate(channel_names) for i,c2 in enumerate(channel_names) if (c1.split(\"-\")[0]==c2.split(\"-\")[1] or c1.split(\"-\")[1]==c2.split(\"-\")[1] \n",
    "          or c1.split(\"-\")[0]==c2.split(\"-\")[0] or c1.split(\"-\")[1]==c2.split(\"-\")[0])]\n",
    "adj=np.zeros((12,12))\n",
    "for i in indices:\n",
    "    adj[i[0]][i[1]]=1\n",
    "adj=tf.constant(adj,dtype=tf.float32)\n",
    "\n",
    "class GATLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self,output_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.Leakyrelu = layers.LeakyReLU(alpha=0.2)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',shape=(input_shape[-1], self.output_dim), initializer='random_normal',trainable=True)\n",
    "        self.a = self.add_weight(name='a',shape=(2*self.output_dim, 1), initializer='random_normal',trainable=True)\n",
    "    \n",
    "    def call(self,input,adj):\n",
    "        H= tf.matmul(input, self.W)\n",
    "        h1=tf.tile(tf.expand_dims(H, axis=1), [1,12,1,1])\n",
    "        h2=tf.tile(tf.expand_dims(H, axis=2), [1,1,12,1])\n",
    "        result =tf.concat([h1 , h2], axis=-1)\n",
    "        e=self.Leakyrelu(tf.squeeze(tf.matmul(result, self.a),axis=-1))\n",
    "        zero_mat= -1e20*tf.ones_like(e)\n",
    "        msked_e=tf.where(adj==1.0,e,zero_mat)\n",
    "        alpha=tf.nn.softmax(msked_e,axis=-1)\n",
    "        HPrime=tf.matmul(alpha,H)\n",
    "        return tf.nn.elu(HPrime)\n",
    "\n",
    "def create_model():\n",
    "    Input= keras.Input(shape=(12,384,1))\n",
    "    regularizer_dense=regularizers.l2(0.0001)\n",
    "\n",
    "    x= layers.Conv2D(32,(1,5),activation='relu',padding='same')(Input)\n",
    "    y= layers.Conv2D(32,(1,7),activation='relu',padding='same')(Input)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x= layers.Conv2D(64,(1,5),activation='relu',padding='same')(x)\n",
    "    y= layers.Conv2D(64,(1,7),activation='relu',padding='same')(x)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x= layers.Conv2D(8,(1,5),activation='relu',padding='same')(x)\n",
    "    y= layers.SpatialDropout2D(0.2)(x)\n",
    "    y= layers.Conv2D(8,(1,7),activation='relu',padding='same')(y)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x= layers.Conv2D(1,(1,5),activation='relu',padding='same')(x)\n",
    "    y= layers.SpatialDropout2D(0.2)(x)\n",
    "    y= layers.Conv2D(1,(1,7),activation='relu',padding='same')(y)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.Reshape((12,24))(x)\n",
    "\n",
    "    x= GATLayer(37)(x,adj)\n",
    "    x= GATLayer(32)(x,adj)\n",
    "    x= GATLayer(16)(x,adj)\n",
    "\n",
    "    x= layers.GlobalAveragePooling1D()(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    x= layers.Dense(32,activation='relu',kernel_regularizer=regularizer_dense)(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    x= layers.Dense(16,activation='relu',kernel_regularizer=regularizer_dense)(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    x= layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs=Input, outputs=x)\n",
    "\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.002)\n",
    "    loss=keras.losses.BinaryFocalCrossentropy(from_logits=False,gamma=2,alpha=0.4,apply_class_balancing=True)\n",
    "    # loss=keras.losses.BinaryCrossentropy(from_logits=False,label_smoothing=0.001)  \n",
    "    kappa=tfa.metrics.CohenKappa(num_classes=2)\n",
    "    fp=keras.metrics.FalsePositives()\n",
    "    tn=keras.metrics.TrueNegatives()\n",
    "    precision = keras.metrics.Precision()\n",
    "    recall = keras.metrics.Recall()\n",
    "    AUROC = keras.metrics.AUC(curve='ROC', name = 'AUROC')\n",
    "    AUPRC = keras.metrics.AUC(curve='PR', name = 'AUPRC')\n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy', AUROC, AUPRC,fp,tn, precision, recall,kappa])   \n",
    "    return model\n",
    "model=create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"GAT_correct_remArt_3/cp_{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path) \n",
    "# cp_callback=keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=False,verbose=0,save_best_only=True,monitor='val_accuracy')  \n",
    "history=model.fit(x_train,y_train,epochs=200,batch_size=512,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"history_GAT_correct_8.jason\", 'w') as f:\n",
    "    pd.DataFrame(history.history).to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history_cv_0.jason','r') as f:\n",
    "    history_1 = pd.read_json(f)\n",
    "with open('history_cv_1.jason','r') as f:\n",
    "    history_2 = pd.read_json(f)\n",
    "with open('history_cv_2.jason','r') as f:\n",
    "    history_3 = pd.read_json(f)\n",
    "with open('history_cv_3.jason','r') as f:\n",
    "    history_4 = pd.read_json(f)\n",
    "with open('history_cv_4.jason','r') as f:\n",
    "    history_5 = pd.read_json(f)\n",
    "\n",
    "np.percentile([history_1['val_accuracy'].max(),history_2['val_accuracy'].max(),history_3['val_accuracy'].max(),history_4['val_accuracy'].max(),history_5['val_accuracy'].max()],[75,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['accuracy','val_accuracy','loss','val_loss','val_AUROC','val_AUPRC','val_precision','val_recall','recall','precision','AUROC','AUPRC']\n",
    "epochs = range(1, 101)\n",
    "fig,ax=plt.subplots(4,3,figsize=(20,20))\n",
    "for r in range(12):\n",
    "    ax[r//3][r%3].plot(epochs,history_2[metrics[r]],label=metrics[r],color='r')\n",
    "    ax[r//3][r%3].set_title(metrics[r])\n",
    "    ax[r//3][r%3].set_xlabel('Epochs')\n",
    "    ax[r//3][r%3].grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BrainNeoCare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
