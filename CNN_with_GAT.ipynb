{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names=[\"Fp1-T3\",\"T3-O1\",\"Fp1-C3\",\"C3-O1\",\"Fp2-C4\",\"C4-O2\",\"Fp2-T4\",\"T4-O2\",\"T3-C3\",\"C3-Cz\",\"Cz-C4\",\"C4-T4\"]\n",
    "indices =[[r,i] for r,c1 in enumerate(channel_names) for i,c2 in enumerate(channel_names) if (c1.split(\"-\")[0]==c2.split(\"-\")[1] or c1.split(\"-\")[1]==c2.split(\"-\")[1] \n",
    "          or c1.split(\"-\")[0]==c2.split(\"-\")[0] or c1.split(\"-\")[1]==c2.split(\"-\")[0])]\n",
    "adj=np.zeros((12,12))\n",
    "for i in indices:\n",
    "    adj[i[0]][i[1]]=1\n",
    "adj=tf.constant(adj,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(tf.random.normal((12,12)),dtype=tf.float32)\n",
    "c=tf.where(adj==1,a,tf.zeros_like(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixTransformationLayer(layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super(MatrixTransformationLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',shape=(input_shape[-1], self.output_dim), initializer='random_normal',trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.W)\n",
    "\n",
    "class AttentionMechanismLayer(layers.Layer):\n",
    "    def __init__(self,adj):\n",
    "        super(AttentionMechanismLayer, self).__init__()\n",
    "        self.adj=adj\n",
    "        self.LeakyReLU = layers.LeakyReLU(alpha=0.2)\n",
    "    def build(self, input_shape):\n",
    "        self.shape0 = input_shape[-2]\n",
    "        self.shape1 = input_shape[-1]\n",
    "        self.a = self.add_weight(name='a',shape=(2*input_shape[-1], 1), initializer='random_normal',trainable=True)\n",
    "    def call(self,input):\n",
    "        h1=tf.tile(tf.expand_dims(input, axis=1), [1,self.shape0, 1, 1])\n",
    "        h2=tf.tile(tf.expand_dims(input, axis=2), [1,1, self.shape0, 1])\n",
    "        result =tf.concat([h1 , h2], axis=-1)\n",
    "        e=self.LeakyReLU(tf.squeeze(tf.matmul(result, self.a),axis=-1))\n",
    "        zero_mat=-1e20*tf.zeros_like(e)\n",
    "        msked_e=tf.where(self.adj==1,e,zero_mat)\n",
    "        alpha=tf.nn.softmax(msked_e,axis=-1)\n",
    "        HPrime=tf.matmul(alpha,input)\n",
    "        return HPrime\n",
    "        \n",
    "\n",
    "Input= keras.Input(shape=(12,1024,1)) \n",
    "x= layers.Conv2D(4,(1,5),activation='relu',padding='same')(Input)\n",
    "x= layers.Conv2D(8,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.MaxPool2D((1,2))(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x= layers.Conv2D(16,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.Conv2D(32,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.MaxPool2D((1,2))(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x= layers.Conv2D(8,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.Conv2D(8,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.MaxPool2D((1,2))(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x= layers.Conv2D(1,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.Conv2D(1,(1,5),activation='relu',padding='same')(x)\n",
    "x= layers.MaxPool2D((1,2))(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x= layers.Reshape((12,64))(x)\n",
    "x = MatrixTransformationLayer(32)(x)\n",
    "x = AttentionMechanismLayer(adj)(x)\n",
    "x = MatrixTransformationLayer(16)(x)\n",
    "x = AttentionMechanismLayer(adj)(x)\n",
    "x = MatrixTransformationLayer(8)(x)\n",
    "x = AttentionMechanismLayer(adj)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "model = keras.Model(inputs=Input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 12, 1024, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_160 (Conv2D)         (None, 12, 1024, 4)       24        \n",
      "                                                                 \n",
      " conv2d_161 (Conv2D)         (None, 12, 1024, 8)       168       \n",
      "                                                                 \n",
      " max_pooling2d_80 (MaxPooli  (None, 12, 512, 8)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_80 (Ba  (None, 12, 512, 8)        32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_162 (Conv2D)         (None, 12, 512, 16)       656       \n",
      "                                                                 \n",
      " conv2d_163 (Conv2D)         (None, 12, 512, 32)       2592      \n",
      "                                                                 \n",
      " max_pooling2d_81 (MaxPooli  (None, 12, 256, 32)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_81 (Ba  (None, 12, 256, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_164 (Conv2D)         (None, 12, 256, 8)        1288      \n",
      "                                                                 \n",
      " conv2d_165 (Conv2D)         (None, 12, 256, 8)        328       \n",
      "                                                                 \n",
      " max_pooling2d_82 (MaxPooli  (None, 12, 128, 8)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_82 (Ba  (None, 12, 128, 8)        32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_166 (Conv2D)         (None, 12, 128, 1)        41        \n",
      "                                                                 \n",
      " conv2d_167 (Conv2D)         (None, 12, 128, 1)        6         \n",
      "                                                                 \n",
      " max_pooling2d_83 (MaxPooli  (None, 12, 64, 1)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_83 (Ba  (None, 12, 64, 1)         4         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " reshape_20 (Reshape)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " matrix_transformation_laye  (None, 12, 32)            2048      \n",
      " r_22 (MatrixTransformation                                      \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " attention_mechanism_layer_  (None, 12, 32)            64        \n",
      " 21 (AttentionMechanismLaye                                      \n",
      " r)                                                              \n",
      "                                                                 \n",
      " matrix_transformation_laye  (None, 12, 16)            512       \n",
      " r_23 (MatrixTransformation                                      \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " attention_mechanism_layer_  (None, 12, 16)            32        \n",
      " 22 (AttentionMechanismLaye                                      \n",
      " r)                                                              \n",
      "                                                                 \n",
      " matrix_transformation_laye  (None, 12, 8)             128       \n",
      " r_24 (MatrixTransformation                                      \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " attention_mechanism_layer_  (None, 12, 8)             16        \n",
      " 23 (AttentionMechanismLaye                                      \n",
      " r)                                                              \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 8)                 0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8099 (31.64 KB)\n",
      "Trainable params: 8001 (31.25 KB)\n",
      "Non-trainable params: 98 (392.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BrainNeoCare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
