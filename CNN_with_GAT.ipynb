{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np \n",
    "from scipy import signal \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_addons as tfa\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('Datasets/GAT/traindata_new.npy',mmap_mode='r')\n",
    "y_train = np.load('Datasets/GAT/trainlabels_new.npy',mmap_mode='r')\n",
    "x_test  = np.load('Datasets/GAT/testdata_new.npy',mmap_mode='r')\n",
    "y_test  = np.load('Datasets/GAT/testlabels_new.npy',mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=x_train.mean()\n",
    "std=x_train.std()\n",
    "\n",
    "x_train=(x_train-mean)/std\n",
    "x_test=(x_test-mean)/std\n",
    "\n",
    "x_train=np.expand_dims(x_train,axis=-1)\n",
    "x_test=np.expand_dims(x_test,axis=-1)\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(train_indices)\n",
    "x_train = x_train[train_indices]\n",
    "y_train = y_train[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model('Saved_models/GAT_paper_model/cp_0175.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for 18 channels \n",
    "adj=tf.constant(\n",
    "    [  [1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "       [0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "       [1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
    "       [0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
    "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.],\n",
    "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
    "    dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(physical_devices[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_names=[\"Fp1-T3\",\"T3-O1\",\"Fp1-C3\",\"C3-O1\",\"Fp2-C4\",\"C4-O2\",\"Fp2-T4\",\"T4-O2\",\"T3-C3\",\"C3-Cz\",\"Cz-C4\",\"C4-T4\"]\n",
    "indices =[[r,i] for r,c1 in enumerate(channel_names) for i,c2 in enumerate(channel_names) if (c1.split(\"-\")[0]==c2.split(\"-\")[1] or c1.split(\"-\")[1]==c2.split(\"-\")[1] \n",
    "          or c1.split(\"-\")[0]==c2.split(\"-\")[0] or c1.split(\"-\")[1]==c2.split(\"-\")[0])]\n",
    "adj=np.zeros((12,12))\n",
    "# adj[0,6],adj[6,0]=1,1\n",
    "# adj[1,7],adj[7,1]=1,1\n",
    "for i in indices:\n",
    "    adj[i[0]][i[1]]=1\n",
    "adj=tf.constant(adj,dtype=tf.float32)\n",
    "\n",
    "class GATLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self,output_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.Leakyrelu = layers.LeakyReLU(alpha=0.2)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',shape=(input_shape[-1], self.output_dim), initializer='random_normal',trainable=True)\n",
    "        self.a = self.add_weight(name='a',shape=(2*self.output_dim, 1), initializer='random_normal',trainable=True)\n",
    "    \n",
    "    def call(self,input,adj):\n",
    "        H= tf.matmul(input, self.W)\n",
    "        h1=tf.tile(tf.expand_dims(H, axis=1), [1,12,1,1])\n",
    "        h2=tf.tile(tf.expand_dims(H, axis=2), [1,1,12,1])\n",
    "        result =tf.concat([h1 , h2], axis=-1)\n",
    "        e=self.Leakyrelu(tf.squeeze(tf.matmul(result, self.a),axis=-1))\n",
    "        zero_mat= -1e20*tf.ones_like(e)\n",
    "        msked_e=tf.where(adj==1.0,e,zero_mat)\n",
    "        alpha=tf.nn.softmax(msked_e,axis=-1)\n",
    "        HPrime=tf.matmul(alpha,H)\n",
    "        return tf.nn.elu(HPrime)\n",
    "\n",
    "# class AttentionLayer(layers.Layer):\n",
    "#     def __init__(self, output_dim):\n",
    "#         super(AttentionLayer, self).__init__()\n",
    "#         self.output_dim = output_dim\n",
    "    \n",
    "#     def build(self, input_shape):\n",
    "#         self.WQ = self.add_weight(name='WQ',shape=(input_shape[-1], self.output_dim), initializer='random_normal',trainable=True)\n",
    "#         self.WK = self.add_weight(name='WK',shape=(input_shape[-1], self.output_dim), initializer='random_normal',trainable=True)\n",
    "#         self.WV = self.add_weight(name='WV',shape=(input_shape[-1], self.output_dim), initializer='random_normal',trainable=True)\n",
    "\n",
    "#     def call(self, input,adj):\n",
    "#         Q = tf.matmul(input, self.WQ)\n",
    "#         K = tf.matmul(input, self.WK)\n",
    "#         V = tf.matmul(input, self.WV)\n",
    "#         e = tf.matmul(Q, K, transpose_b=True)\n",
    "#         e = e / tf.math.sqrt(tf.cast(self.output_dim, tf.float32))\n",
    "#         zero_mat= -1e20*tf.ones_like(e)\n",
    "#         msked_e=tf.where(adj==1.0,e,zero_mat)\n",
    "#         alpha = tf.nn.softmax(msked_e, axis=-1)\n",
    "#         H = tf.matmul(alpha, V)\n",
    "#         return H\n",
    "\n",
    "def create_model():\n",
    "    Input= keras.Input(shape=(12,384,1))\n",
    "    regularizer_dense=regularizers.l2(0.0001)\n",
    "\n",
    "    x= layers.Conv2D(32,(1,5),activation='relu',padding='same')(Input)\n",
    "    y= layers.Conv2D(32,(1,7),activation='relu',padding='same')(Input)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x= layers.Conv2D(64,(1,5),activation='relu',padding='same')(x)\n",
    "    y- layers.Conv2D(64,(1,7),activation='relu',padding='same')(x)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x= layers.Conv2D(8,(1,5),activation='relu',padding='same')(x)\n",
    "    y= layers.Conv2D(8,(1,7),activation='relu',padding='same')(x)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x= layers.Conv2D(1,(1,5),activation='relu',padding='same')(x)\n",
    "    y= layers.Conv2D(1,(1,7),activation='relu',padding='same')(x)\n",
    "    x= layers.add([x,y])\n",
    "    x= layers.AveragePooling2D((1,2))(x)\n",
    "    x= layers.Reshape((12,24))(x)\n",
    "\n",
    "    x= GATLayer(37)(x,adj)\n",
    "    x= GATLayer(32)(x,adj)\n",
    "    x= GATLayer(16)(x,adj)\n",
    "    \n",
    "    x= layers.GlobalAveragePooling1D()(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    x= layers.Dense(32,activation='relu',kernel_regularizer=regularizer_dense)(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    x= layers.Dense(16,activation='relu',kernel_regularizer=regularizer_dense)(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    x= layers.Dense(1,activation='sigmoid',kernel_regularizer=regularizer_dense)(x)\n",
    "\n",
    "    model = keras.Model(inputs=Input, outputs=x)\n",
    "\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.002)\n",
    "    loss=keras.losses.BinaryFocalCrossentropy(from_logits=False,gamma=2,alpha=0.4,apply_class_balancing=True)\n",
    "    kappa=tfa.metrics.CohenKappa(num_classes=2)\n",
    "    fp=keras.metrics.FalsePositives()\n",
    "    tn=keras.metrics.TrueNegatives()\n",
    "    precision = keras.metrics.Precision(name='precision')\n",
    "    recall = keras.metrics.Recall(name='recall')\n",
    "    AUROC = keras.metrics.AUC(curve='ROC', name = 'AUROC')\n",
    "    AUPRC = keras.metrics.AUC(curve='PR', name = 'AUPRC')\n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy', AUROC, AUPRC,fp,tn, precision, recall,kappa])   \n",
    "    return model\n",
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"GAT_new/cp_{epoch:02d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path) \n",
    "cp_callback=keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=False,verbose=0,save_best_only=True,monitor='val_AUROC',mode='max')  \n",
    "history=model.fit(x_train,y_train,epochs=100,batch_size=512,verbose=1,validation_data=(x_test,y_test),callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Attention.jason\", 'w') as f:\n",
    "    pd.DataFrame(history.history).to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./History/history_paper_cv_0.jason\", 'r') as f:\n",
    "    history1=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_1.jason\", 'r') as f:\n",
    "    history2=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_2.jason\", 'r') as f:\n",
    "    history3=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_3.jason\", 'r') as f:\n",
    "    history4=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_4.jason\", 'r') as f:\n",
    "    history5=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_5.jason\", 'r') as f:\n",
    "    history6=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_6.jason\", 'r') as f:\n",
    "    history7=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_7.jason\", 'r') as f:\n",
    "    history8=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_8.jason\", 'r') as f:\n",
    "    history9=pd.read_json(f)\n",
    "with open(\"./History/history_paper_cv_9.jason\", 'r') as f:\n",
    "    history10=pd.read_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['accuracy','val_accuracy','loss','val_loss','val_AUROC','val_AUPRC','val_recall_9','val_cohen_kappa']\n",
    "\n",
    "epochs = range(1, 51)\n",
    "fig,ax=plt.subplots(4,2,figsize=(20,20))\n",
    "for r in range(8):\n",
    "    ax[r//2][r%2].plot(history5[metrics[r]],color='r',label='inter')\n",
    "    ax[r//2][r%2].plot(history10[metrics[r]],color='b',label='old')\n",
    "    ax[r//2][r%2].set_title(metrics[r])\n",
    "    ax[r//2][r%2].set_xlabel('Epochs')\n",
    "    ax[r//2][r%2].legend()\n",
    "    ax[r//2][r%2].grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Attention.jason\", 'r') as f:\n",
    "    history1=pd.read_json(f)\n",
    "\n",
    "with open(\"History/history_cv_0.jason\", 'r') as f:\n",
    "    history2=pd.read_json(f)\n",
    "\n",
    "plt.plot(history1['accuracy'],color='r',label=' Accuracy with dot product attention')\n",
    "plt.plot(history2['accuracy'],color='b',label=' Accuracy with GAT')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "# plt.ylim(0,1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BrainNeoCare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
