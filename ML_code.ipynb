{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUVWvOpCMLT3",
        "outputId": "2d60d08e-e8df-432b-caed-2ca13ec78d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdY2ESyzDlda",
        "outputId": "3e4e3c66-d41f-4998-d313-09e6bfef659d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "! pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVAuWFnJEoIy",
        "outputId": "0b5709e7-0124-4e98-f240-941dbee7cc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.12.0\n",
            "Running on TPU  ['10.49.77.178:8470']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1GYPwzDldb"
      },
      "source": [
        "## Load the Data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-e1-yTpanT3"
      },
      "outputs": [],
      "source": [
        "def parse(serialized,signal_shape=(1024,12),label_shape=(1,)):\n",
        "\n",
        "    features = {'signal': tf.io.FixedLenFeature([], tf.string), 'label': tf.io.FixedLenFeature([], tf.int64)}\n",
        "    parsed_example = tf.io.parse_single_example(serialized=serialized, features=features)\n",
        "\n",
        "    signal = parsed_example['signal']\n",
        "    label = parsed_example['label']\n",
        "\n",
        "    signal = tf.io.decode_raw(signal, tf.float64)\n",
        "    signal = tf.reshape(signal, shape=signal_shape)\n",
        "    signal = tf.transpose(signal)\n",
        "    signal = signal*(10**5)\n",
        "\n",
        "    # label = tf.io.decode_raw(label, tf.int64)\n",
        "    label = tf.reshape(label, shape=label_shape)\n",
        "\n",
        "    return signal, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sADnBx-fb0JA"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "train_files = tf.io.matching_files('gs://tinyml/train_fyp/*train.tfrec')\n",
        "train_files = tf.random.shuffle(train_files)\n",
        "shards = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "train_dataset = shards.interleave(lambda x: tf.data.TFRecordDataset(x), num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
        "train_dataset = train_dataset.map(parse, num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(128)\n",
        "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "test_files = tf.io.matching_files('gs://tinyml/test_fyp/*test.tfrec')\n",
        "shards = tf.data.Dataset.from_tensor_slices(test_files)\n",
        "test_dataset = shards.interleave(lambda x: tf.data.TFRecordDataset(x), num_parallel_calls=AUTOTUNE)\n",
        "test_dataset = test_dataset.map(parse, num_parallel_calls=AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(128)\n",
        "test_dataset = test_dataset.prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1pt8zjca4Dd",
        "outputId": "40b0a6d5-a265-45a2-e299-98e08ffd98c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function Executor.__del__ at 0x7f660c5288b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
            "    self.wait()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5325 61392\n"
          ]
        }
      ],
      "source": [
        "# for i in train_dataset:\n",
        "#   print(i[0][0].shape)\n",
        "#   print(i[1][0][0])\n",
        "#   plt.plot(i[0][0][:,0])\n",
        "#   break\n",
        "\n",
        "pos = 0\n",
        "tot = 0\n",
        "for i in train_dataset:\n",
        "  # print(i[1].shape)\n",
        "  pos+=sum(i[1])\n",
        "  tot+=i[1].shape[0]\n",
        "  # break\n",
        "\n",
        "pos = pos.numpy()[0]\n",
        "print(pos, tot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7PYkVAhDldd"
      },
      "source": [
        "## Develope a large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCJROd1eDldd"
      },
      "outputs": [],
      "source": [
        "def block_type1(n_filters1,n_filters2, kernel, pad, x):\n",
        "    block1=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(x)\n",
        "    block1=layers.BatchNormalization()(block1)\n",
        "    block1=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(block1)\n",
        "    block1=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(block1)\n",
        "    block1=layers.BatchNormalization()(block1)\n",
        "    block1=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(block1)\n",
        "    block1=layers.Conv1D(n_filters2,kernel,padding=pad,activation='swish')(block1)\n",
        "    block1=layers.BatchNormalization()(block1)\n",
        "\n",
        "    block1_o=layers.Conv1D(n_filters2,kernel,padding=pad,activation='swish')(x)\n",
        "    block1_o=layers.BatchNormalization()(block1_o)\n",
        "\n",
        "    added1=layers.add([block1,block1_o])\n",
        "    return layers.Conv1D(n_filters2,kernel,padding=pad,activation='swish')(added1)\n",
        "\n",
        "def block_type2(n_filters1,n_filters2, kernel, pad, x):\n",
        "    block=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(x)\n",
        "    block=layers.BatchNormalization()(block)\n",
        "    block=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(block)\n",
        "    block=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(block)\n",
        "    block=layers.BatchNormalization()(block)\n",
        "    block=layers.Conv1D(n_filters1,kernel,padding=pad,activation='swish')(block)\n",
        "    block=layers.Conv1D(n_filters2,kernel,padding=pad,activation='swish')(block)\n",
        "    block=layers.BatchNormalization()(block)\n",
        "\n",
        "    added=layers.add([x,block])\n",
        "    return layers.Conv1D(n_filters2,kernel,padding=pad,activation='swish')(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjN9LDPUDldd",
        "outputId": "573ff491-71c1-4d9c-915a-9fe2d78af63d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input_signal (InputLayer)      [(None, 1024, 12)]   0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 1024, 32)     1184        ['Input_signal[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1024, 32)    128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 1024, 32)     3104        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 512, 32)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 512, 32)      3104        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 512, 32)     128         ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 512, 32)      3104        ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 512, 32)      3104        ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 512, 32)     128         ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 512, 32)      3104        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 512, 128)     12416       ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 512, 128)     12416       ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 512, 128)    512         ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 512, 128)    512         ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 512, 128)     0           ['batch_normalization_3[0][0]',  \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 512, 128)     49280       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 512, 32)      12320       ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 512, 32)     128         ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 512, 32)      3104        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 512, 32)      3104        ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 512, 32)     128         ['conv1d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 512, 32)      3104        ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 512, 128)     12416       ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 512, 128)    512         ['conv1d_13[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 512, 128)     0           ['conv1d_8[0][0]',               \n",
            "                                                                  'batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 512, 128)     49280       ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 512, 32)      12320       ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 512, 32)     128         ['conv1d_15[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 512, 32)      3104        ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 512, 32)      3104        ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 512, 32)     128         ['conv1d_17[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 512, 32)      3104        ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 512, 128)     12416       ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 512, 128)    512         ['conv1d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 512, 128)     0           ['conv1d_14[0][0]',              \n",
            "                                                                  'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 512, 128)     49280       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 128, 128)    0           ['conv1d_20[0][0]']              \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 128, 64)      57408       ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 128, 64)     256         ['conv1d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 128, 64)      28736       ['conv1d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 128, 64)     256         ['conv1d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 128, 256)     114944      ['conv1d_24[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 128, 256)     229632      ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 128, 256)    1024        ['conv1d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 128, 256)    1024        ['conv1d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 128, 256)     0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 128, 256)     459008      ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 128, 64)      114752      ['conv1d_27[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 128, 64)     256         ['conv1d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 128, 64)      28736       ['conv1d_29[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 128, 64)     256         ['conv1d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 128, 256)     114944      ['conv1d_31[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 128, 256)    1024        ['conv1d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 128, 256)     0           ['conv1d_27[0][0]',              \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 128, 256)     459008      ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 128, 64)      114752      ['conv1d_33[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 128, 64)     256         ['conv1d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 128, 64)      28736       ['conv1d_35[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 128, 64)     256         ['conv1d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 128, 256)     114944      ['conv1d_37[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 128, 256)    1024        ['conv1d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 128, 256)     0           ['conv1d_33[0][0]',              \n",
            "                                                                  'batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 128, 256)     459008      ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 128, 64)      114752      ['conv1d_39[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 128, 64)     256         ['conv1d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 128, 64)      28736       ['conv1d_41[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 128, 64)     256         ['conv1d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 128, 64)      28736       ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 128, 256)     114944      ['conv1d_43[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 128, 256)    1024        ['conv1d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 128, 256)     0           ['conv1d_39[0][0]',              \n",
            "                                                                  'batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 128, 256)     459008      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 256)         0           ['conv1d_45[0][0]']              \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            257         ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,539,777\n",
            "Trainable params: 3,534,721\n",
            "Non-trainable params: 5,056\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "  Input=keras.Input(shape=(1024,12),name=\"Input_signal\")\n",
        "  x=layers.Conv1D(32,3,padding=\"same\",activation='swish')(Input)\n",
        "  x=layers.BatchNormalization()(x)\n",
        "  x=layers.Conv1D(32,3,padding=\"same\",activation='swish')(x)\n",
        "  x=layers.MaxPool1D(2)(x)\n",
        "\n",
        "  x1=block_type1(32,128,3,\"same\",x)\n",
        "  x2=block_type2(32,128,3,\"same\",x1)\n",
        "  x3=block_type2(32,128,3,\"same\",x2)\n",
        "  x3=layers.AveragePooling1D(4)(x3)\n",
        "  x4=block_type1(64,256,7,\"same\",x3)\n",
        "  x5=block_type2(64,256,7,\"same\",x4)\n",
        "  x6=block_type2(64,256,7,\"same\",x5)\n",
        "  x7=block_type2(64,256,7,\"same\",x6)\n",
        "  # x7=layers.AveragePooling1D(4)(x7)\n",
        "  # x8=block_type1(128,512,9,\"same\",x7)\n",
        "  # x9=block_type2(128,512,9,\"same\",x8)\n",
        "  # x10=block_type2(128,512,9,\"same\",x9)\n",
        "  # x11=block_type2(128,512,9,\"same\",x10)\n",
        "  # x12=block_type2(128,512,9,\"same\",x11)\n",
        "  # x13=block_type2(128,512,9,\"same\",x12)\n",
        "  # x13=layers.AveragePooling1D(4)(x13)\n",
        "  # x14=block_type1(256,1024,11,\"same\",x13)\n",
        "  # x15=block_type2(256,1024,11,\"same\",x14)\n",
        "  # x16=block_type2(256,1024,11,\"same\",x15)\n",
        "  x16=layers.GlobalAveragePooling1D()(x7)\n",
        "  x16=layers.Dense(1)(x16)\n",
        "\n",
        "  model=keras.Model(Input,x16)\n",
        "  F1 = tfa.metrics.FBetaScore(num_classes=1)\n",
        "  AUROC = tf.keras.metrics.AUC(curve='ROC', name = 'AUROC')\n",
        "  AUPRC = tf.keras.metrics.AUC(curve='PR', name = 'AUPRC')\n",
        "  model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy', F1, AUROC, AUPRC])\n",
        "  keras.utils.plot_model(model,'model.png',show_layer_activations=True,show_shapes=True)\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "  model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtDYdil2l8kB"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    i = keras.Input((1024,12), name='signal')\n",
        "    x = layers.Conv1D(filters=72, kernel_size=15, activation='swish', padding='same')(i)\n",
        "    a = layers.Conv1D(filters=72, kernel_size=1, padding='same')(i)\n",
        "    a = layers.Add()([a,x])\n",
        "    a = layers.Activation('swish')(a)\n",
        "    x = layers.SpatialDropout1D(0.2)(a)\n",
        "    b = layers.AveragePooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=144, kernel_size=3, activation='swish', padding='same')(b)\n",
        "    a = layers.Conv1D(filters=144, kernel_size=1, padding='same')(b)\n",
        "    a = layers.Add()([a,x])\n",
        "    a = layers.Activation('swish')(a)\n",
        "    x = layers.SpatialDropout1D(0.2)(a)\n",
        "    b = layers.AveragePooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=288, kernel_size=5, activation='swish', padding='same')(b)\n",
        "    a = layers.Conv1D(filters=288, kernel_size=1, padding='same')(b)\n",
        "    a = layers.Add()([a,x])\n",
        "    a = layers.Activation('swish')(a)\n",
        "    x = layers.SpatialDropout1D(0.2)(a)\n",
        "    b = layers.AveragePooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=576, kernel_size=7, activation='swish', padding='same')(b)\n",
        "    a = layers.Conv1D(filters=576, kernel_size=1, padding='same')(b)\n",
        "    a = layers.Add()([a,x])\n",
        "    a = layers.Activation('swish')(a)\n",
        "    b = layers.SpatialDropout1D(0.2)(a)\n",
        "    x = layers.GlobalAveragePooling1D()(b)\n",
        "\n",
        "    # j = keras.Input((2000,2*2), name='fft')\n",
        "    # y = layers.Conv1D(filters=72, kernel_size=3, activation='swish', padding='same')(j)\n",
        "    # y = layers.SpatialDropout1D(0.1)(y)\n",
        "    # y = layers.AveragePooling1D(pool_size=2)(y)\n",
        "    # y = layers.Conv1D(filters=144, kernel_size=5, activation='swish', padding='same')(y)\n",
        "    # y = layers.SpatialDropout1D(0.1)(y)\n",
        "    # y = layers.AveragePooling1D(pool_size=2)(y)\n",
        "    # y = layers.Conv1D(filters=288, kernel_size=7, activation='swish', padding='same')(y)\n",
        "    # y = layers.SpatialDropout1D(0.1)(y)\n",
        "    # y = layers.AveragePooling1D(pool_size=2)(y)\n",
        "    # y = layers.Conv1D(filters=576, kernel_size=9, activation='swish', padding='same')(y)\n",
        "    # y = layers.SpatialDropout1D(0.1)(y)\n",
        "    # y = layers.AveragePooling1D(pool_size=2)(y)\n",
        "    # y = layers.Conv1D(filters=1152, kernel_size=11, activation='swish', padding='same')(y)\n",
        "    # y = layers.SpatialDropout1D(0.1)(y)\n",
        "    # y = layers.GlobalAveragePooling1D()(y)\n",
        "\n",
        "    # f = layers.Concatenate()([x,y])\n",
        "    # f = layers.Activation('swish')(f)\n",
        "\n",
        "    f = layers.Dense(576, activation='swish')(x)\n",
        "    f = layers.Dropout(0.5)(f)\n",
        "    f = layers.Dense(1,activation = 'sigmoid')(f)\n",
        "\n",
        "    F1 = tfa.metrics.FBetaScore(num_classes=1)\n",
        "    AUROC = tf.keras.metrics.AUC(curve='ROC', name = 'AUROC')\n",
        "    AUPRC = tf.keras.metrics.AUC(curve='PR', name = 'AUPRC')\n",
        "    model = keras.Model(i,f)\n",
        "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', AUROC, AUPRC])\n",
        "    tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=False,show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96,layer_range=None, show_layer_activations=False)\n",
        "    return model\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "  model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "7c7E69ZeUIvp",
        "outputId": "75906a84-6ac7-4d70-eeb3-255160b09409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-dbf2445f19de>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_for_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_for_1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 1024, 12), found shape=(None, 12, 1024)\n"
          ]
        }
      ],
      "source": [
        "weight_for_0 = (1 / (tot-pos)) * (tot / 2.0)\n",
        "weight_for_1 = (1 / pos) * (tot / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "model.fit(train_dataset, validation_data = test_dataset, epochs=10, class_weight=class_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM-kE-RxDldd"
      },
      "source": [
        "## Small model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkxapcg1Dldd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxoHTqMwDlde"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import tensorflow_addons as tfa\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "model=keras.Sequential(name=\"BrainNeoCare_CNN\")\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(12,1024,1),name=\"Input_conv2D\"))\n",
        "model.add(keras.layers.MaxPooling2D((1,2),name=\"Maxpooling2D_1\"))\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu',name=\"Conv2D_2\"))\n",
        "model.add(keras.layers.MaxPooling2D((1,2),name=\"Maxpooling2D_2\"))\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(1,3), activation='relu',name=\"Conv2D_3\"))\n",
        "model.add(keras.layers.MaxPooling2D((1,2),name=\"Maxpooling2D_3\"))\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(1,3), activation='relu',name=\"Conv2D_4\"))\n",
        "model.add(keras.layers.AveragePooling2D((1,2),name=\"AveragePooling2D_1\"))\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu',name=\"2D_F_Con\"))\n",
        "model.add(keras.layers.AveragePooling2D((1,2),name=\"AveragePooling2D_2\"))\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu',name=\"2D_F_Con_2\"))\n",
        "model.add(keras.layers.MaxPooling2D((1,2),name=\"AveragePooling2D_3\"))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(128, activation='relu'))\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "model.add(keras.layers.Dense(8, activation='relu') )\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euPWYvojDlde"
      },
      "outputs": [],
      "source": [
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, name=\"binary_crossentropy\",)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pj72EYWDlde"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train,Y_train,epochs=10,batch_size=32,validation_split=0.2,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu2_WyefTutF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwRkXT2GTumK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRlUwmaqHmIa"
      },
      "outputs": [],
      "source": [
        "x=np.load('/content/drive/MyDrive/FYP_ML/zenodo_data.npy', mmap_mode='r')\n",
        "y=np.load('/content/drive/MyDrive/FYP_ML/zenodo_labels.npy', mmap_mode='r')\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb2ZIS84g6O8"
      },
      "outputs": [],
      "source": [
        "y_train_positive_pos=np.where(y_train==1)\n",
        "y_test_positive_pos=np.where(y_test==1)\n",
        "\n",
        "x_train_positive=x_train[y_train_positive_pos]\n",
        "x_test_positive=x_test[y_test_positive_pos]\n",
        "\n",
        "y_train_negative_pos=np.where(y_train==0)\n",
        "y_test_negative_pos=np.where(y_test==0)\n",
        "\n",
        "x_train_negative=x_train[y_train_negative_pos][0:6000,...]\n",
        "x_test_negative=x_test[y_test_negative_pos][0:2000,...]\n",
        "\n",
        "new_x_train=np.concatenate((x_train_positive,x_train_negative),axis=0)\n",
        "new_y_train=np.concatenate((np.ones((x_train_positive.shape[0],1)),np.zeros((x_train_negative.shape[0],1))),axis=0)\n",
        "\n",
        "new_x_test=np.concatenate((x_test_positive,x_test_negative),axis=0)\n",
        "new_y_test=np.concatenate((np.ones((x_test_positive.shape[0],1)),np.zeros((x_test_negative.shape[0],1))),axis=0)\n",
        "\n",
        "np.save('/content/drive/MyDrive/FYP_ML/balanced_x_train.npy',new_x_train)\n",
        "np.save('/content/drive/MyDrive/FYP_ML/balanced_y_train.npy',new_y_train)\n",
        "\n",
        "np.save('/content/drive/MyDrive/FYP_ML/balanced_x_test.npy',new_x_test)\n",
        "np.save('/content/drive/MyDrive/FYP_ML/balanced_y_test.npy',new_y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUFbvKHLONQf"
      },
      "outputs": [],
      "source": [
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ahMRqJVnW0"
      },
      "outputs": [],
      "source": [
        "def serialize_example(signal, label):\n",
        "  feature = {\n",
        "      'signal': _bytes_feature(signal),\n",
        "      'label':_int64_feature(label)\n",
        "  }\n",
        "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  return example_proto.SerializeToString()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIRoVTz6WTAE",
        "outputId": "343d6ad2-c3e6-4c5d-fd73-68be5aa35b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Writing TFRecord 0 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://00-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "Copying file://04-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "- [2/2 files][530.1 MiB/530.1 MiB] 100% Done                                    \n",
            "Operation completed over 2 objects/530.1 MiB.                                    \n",
            "\n",
            "Writing TFRecord 1 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://01-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "| [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 2 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://02-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 3 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://03-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 4 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://04-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "/ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 5 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://05-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "-\n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 6 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://06-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "/ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 7 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://07-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 8 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://08-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 9 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://09-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 10 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://10-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 11 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://11-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 12 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://12-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 13 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://13-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 14 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://14-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 15 of 16...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://15-3837_train.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "|\n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "CT = 16\n",
        "SIZE = len(x_train)//CT\n",
        "\n",
        "for j in range(CT):\n",
        "    print(); print('Writing TFRecord %i of %i...'%(j,CT))\n",
        "    CT2 = min(SIZE,len(x_train)-j*SIZE)\n",
        "    with tf.io.TFRecordWriter('%.2i-%i_train.tfrec'%(j,CT2)) as writer:\n",
        "        for k in range(CT2):\n",
        "            signal = x_train[SIZE*j+k].tobytes()\n",
        "            label = y_train[SIZE*j+k]\n",
        "            example = serialize_example(signal,label)\n",
        "            writer.write(example)\n",
        "            if k%100==0: print(k,', ',end='')\n",
        "    !gsutil -m cp -r *train.tfrec gs://tinyml/train_fyp/\n",
        "    !rm *train.tfrec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl1orIvmaIrm",
        "outputId": "1507d137-b14d-45b6-f0c1-ba1d870cd5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Writing TFRecord 0 of 4...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://00-3837_test.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 1 of 4...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://01-3837_test.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 2 of 4...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://02-3837_test.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n",
            "\n",
            "Writing TFRecord 3 of 4...\n",
            "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , Copying file://03-3837_test.tfrec [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [1/1 files][359.9 MiB/359.9 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/359.9 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "CT = 4\n",
        "SIZE = len(x_test)//CT\n",
        "\n",
        "for j in range(CT):\n",
        "    print(); print('Writing TFRecord %i of %i...'%(j,CT))\n",
        "    CT2 = min(SIZE,len(x_test)-j*SIZE)\n",
        "    with tf.io.TFRecordWriter('%.2i-%i_test.tfrec'%(j,CT2)) as writer:\n",
        "        for k in range(CT2):\n",
        "            signal = x_test[SIZE*j+k].tobytes()\n",
        "            label = y_test[SIZE*j+k]\n",
        "            example = serialize_example(signal,label)\n",
        "            writer.write(example)\n",
        "            if k%100==0: print(k,', ',end='')\n",
        "    !gsutil -m cp -r *test.tfrec gs://tinyml/test_fyp/\n",
        "    !rm *test.tfrec"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
